# ğŸ©º Diabetes Prediction using Machine Learning
A **classification model** to predict diabetes based on patient medical attributes using **Logistic Regression, Random Forest, Decision Trees, and Support Vector Machine **.  
The project includes **data preprocessing, feature selection (RFE), class balancing (SMOTE), and model evaluation** using Precision-Recall, ROC-AUC curves, and Confusion Matrices.

ğŸš€ **Best Model:** Decision Tree (Highest Accuracy & Recall)  
ğŸ“Š **Tech Stack:** Python, Scikit-Learn, Pandas, NumPy, Matplotlib, Seaborn

---

## ğŸ“Œ Features:
âœ… **Data Preprocessing** (Handling missing values, outliers, and scaling)  
âœ… **Feature Selection** using Recursive Feature Elimination (RFE)  
âœ… **Class Balancing** with SMOTE  
âœ… **Multiple Model Training & Evaluation**  
âœ… **Model Performance Analysis** (Confusion Matrix, Precision-Recall, ROC-AUC)  
âœ… **Interactive Streamlit App for Predictions**  

---

## ğŸ“Š Dataset:
ğŸ“Œ **Pima Indians Diabetes Database**  
ğŸ“‚ **Source:** [Kaggle Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)  
- **Contains:** 768 patient records with 8 medical attributes  
- **Target Variable:** `Outcome` (1 = Diabetic, 0 = Non-Diabetic)

| Feature               | Description |
|-----------------------|-------------|
| Pregnancies          | Number of times pregnant |
| Glucose              | Plasma glucose concentration |
| BloodPressure        | Diastolic blood pressure (mm Hg) |
| SkinThickness        | Triceps skin fold thickness (mm) |
| Insulin              | 2-Hour serum insulin (mu U/ml) |
| BMI                  | Body mass index (weight/heightÂ²) |
| DiabetesPedigreeFunction | Diabetes hereditary score |
| Age                  | Age in years |
| Outcome (Target)     | 1 = Diabetes, 0 = No Diabetes |

---

## **1ï¸âƒ£ Data Preprocessing**
ğŸ”¹ Checking for missing values.  
ğŸ”¹ Removed **outliers in Insulin** using the **IQR method**.  
![BoxPlot](images/BoxPlot.png)
ğŸ”¹ **Standardized features** using **Z-score Normalization (StandardScaler)**.

---

## **2ï¸âƒ£ Exploratory Data Analysis (EDA)**  
ğŸ”¹ **Heatmap of feature correlations** to check feature importance.
![Heatplot](images/Heatplot.png)

ğŸ”¹ **Pairplot Analysis** to understand Feature Relationships
![OutcomeVariable](images/OutcomeVariable.png)

ğŸ”¹ **Class Distribution** to to identify data imbalance
![Pairplot](images/Pairplot.png)

---
1ï¸âƒ£ 2ï¸âƒ£ 3ï¸âƒ£ 4ï¸âƒ£ 5ï¸âƒ£ 6ï¸âƒ£ 7ï¸âƒ£ 8ï¸âƒ£
## **3ï¸âƒ£ Feature Selection using RFE**
### âœ… What is RFE?  
- **Recursive Feature Elimination (RFE)** is a feature selection technique that **iteratively removes less important features** to improve model performance.
- It helps in **identifying the most relevant features** while reducing dimensionality and noise.

---

### âœ… Why Use RFE?  
âœ” **Improves Model Generalization** â€“ Reduces overfitting by selecting only the most important features.  
âœ” **Enhances Model Efficiency** â€“ Reducing features improves training speed and simplifies the model.  
âœ” **Boosts Interpretability** â€“ Helps understand which features contribute most to predictions.  

---

### âœ… Features Selected by RFE:
- **Pregnancies**
- **Glucose**
- **BloodPressure**
- **BMI**
- **DiabetesPedigreeFunction**
  
---

## **4ï¸âƒ£ Model Training & Parameter Tuning**
ğŸ”¹ **Trained multiple models:**  
- Logistic Regression  
- Decision Tree  
- Random Forest  
- Support Vector Machine
  
---

ğŸ”¹ **Hyperparameter tuning using GridSearchCV**  
- **Logistic Regression:** Tuned `C`, `max_iter`.  
- **Decision Tree & Random Forest:** Tuned `max_depth`, `min_samples_split`.
- **Support Vector Machine (SVM):** Tuned `C`, `kernel`, `gamma`.

---


---
## **ğŸ“Š Results & Performance Metrics**
ğŸ”¹ **Confusion Matrix** to analyze True Positives & False Negatives.
![ConfusionMatrices](images/ConfusionMatrices.png)
ğŸ”¹ **Precision-Recall & ROC-AUC curves** for model evaluation.  
![PrecisionRecall](images/Precision-Recall.png)
![ROC-AUC](images/ROC-AUC.png)

ğŸ“Œ **Best Model:** **Logistic Regression (AUC = 0.82), Random Forest (AUC = 0.80)** 


---

## **ğŸ“Œ Conclusion**
âœ… **Best Model:** Logistic Regression (AUC = 0.82).  
âœ… **Feature selection (RFE) improved accuracy**.  
âœ… **Using SMOTE helped in balancing dataset**.  

ğŸ“Œ **Future Work:**  
- Experiment with **XGBoost, Deep Learning** to improve model accuracy

---

## ğŸ“œ License
This project is **open-source** under the **MIT License**.

---
